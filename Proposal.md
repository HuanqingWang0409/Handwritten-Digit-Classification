# Final Project Proposal
## Titile：Digit Classification using PCA, Logistic Regression and Deep Neural Networks

## Project Description
### Introduction to the MNIST Database
The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The MNIST database contains 60,000 training images and 10,000 testing images. We will use this data set to compare performance of various classification techniques. The methods we will use are: logistic regression, principal component analysis and neural network.

### Principal Component Analysis (PCA)
Often, data with many features is really generated by a process with only a handful of parameters. Principal component analysis (PCA) is extensionally used for taking a high-dimensional data set and embedding it in a lower dimension.  PCA defines the best d-dimensional subspace as the subspace with the larges variance in the data. The rationale is that, higher spread of data along a direction implies more information from data.

The images in MNIST were converted into the same 28x28 pixel format. This means each image has 784 dimensions to analysis. PCA will be used to reduce dimensions, meanwhile keeping a low mis-classification rate.

### Logistic Regression  
Multinomial logistic regression is a linear model for multi-class classiﬁcation. The conditional probabilities of the outcome class $c∈C$ are modeled using the softmax function:  
$$P(y=c|x,\beta_c,b_c)=\frac{\exp{(\beta_cx+b_c)}}{\Sigma_{d=1}^{C}\exp{(\beta_dx+b_d)}}$$



### Neural Network
This project also implements Convolutional Neural Network (CNN) with PyTorch, which extensively uses tensor operations. CNN is a class of neural networks that is heavily used to analyze visual imagery. The architecture of CNN is convenient for image classification because it takes advantage of the hierarchical pattern in image data. In this case, we will use the CNN model to classify hand written digits and compare its results to PCA and logistic regression.



## Plan of Implementation  
## Principal Component Analysis
### Step 1: Data Preprocessing
Before applying PCA, it is necessary to standardize each variable in the data set into a 0 mean and 1 variance variable for further analysis. 

### Step 2: Compute Covariance Matrix
For a 0-mean variable $X$, its population variance can be calculated as $\frac{1}{n}X^TX$. But in practice, if $X$ is ill-conditioned, then $X^TX$ will be even more ill-conditioned. Therefore, we will use SVD instead: if $X=U\Sigma V^T$, then $X^TX = V\Sigma^2V^T$.

### Step 3: Compute Principal Components
From the formula above, singular vectors in the matrix $V$ are the eigenvectors of $X^TX$, which are call the principal components of $X$. If the singular values $\Sigma_{ii}$ are in descending order, then the $i^{th}$ singular vector in $V$ is the $i^{th}$ principal component.

### Step 4: Plot (Only for 2 Principal Components)
One can plot the image classes with 2 principal components. Non-overlapping classes would imply that the PCA separates out classes successfully.  

## Logistic Regression  
Train the model on the training set, evaluate the model on validation set, and calculate the error rate.   
We use the `sklearn.linear` model implementation of multiclass logistic regression. This implementation of logistic regression assumes a regularization term and $C=\frac{1}{\lambda}$. A small regularization term corresponds to very large $C$. We would try a few C’s with a wide range of orders of magnitude to pick the optimal $C$.  Once the optimal $C$ is picked, we would retrain with the training and development set together.  
Also, we would determine how well PCA works in terms of classiﬁcation by using the PC’s computed to train a logistic regression classifier to predict the class label given those principal components.  

## Neural Network
To ensure the depth of the neural network, the CNN model is trained using batch normalization and Residual Connection. We will also tune learning rate and momentum to obtain the optimal model. Tunning momentum is relatively easy: we will try 0.9 and 0.99 and see which one performs better combining with the learning rate. Tuning the learning rate is a bit trickier because large learning rate corresponds to large noise level but a learning rate that is too small makes the training less efficient. We will use grid search to find the optimal parameters of our model.

## Tests

### Principal Component Analysis
Random forest will be used to classify test images. We will first use random forest without PCA, and then use random forest with PCA. We will select a proper number of components to be used in PCA such that data dimension can be reduced a lot from 784, meanwhile maintaining a low mis-classification rate.

### Logistic Regression  
After setting up the basic work ﬂow use the development set to select the optimal number of principal components $k$ and the optimal $C$, we would retrain the model using both training and validation set and compute the error rate on testing data.  

### Neural Network
Learning rate and momentum are the two hyper-parameters tuned in the CNN model. We will select the model with the smallest validation error.

### Method Comparison
At last, we will compare the mis-classification rates among the three models chosen above: logistic regression, PCA and neural network.
